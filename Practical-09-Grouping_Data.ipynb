{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:450px\">Practical 9: Grouping Data</h1>\n",
    "    <h2 style=\"width:450px\">Aggregation, Classification &amp; Clustering</h2>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: _this_ group of observations are similar to one another, _that_ group is dissimilar to this group. Sometimes we have a *label* that we can use as part of the process (in which case we're doing **classification**), and somtimes we don't (in which case we're doing **clustering**). But what defines similarity and difference? There is no _one_ answer to that question and so there are many different ways to cluster or classify data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import os\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# All of these are potentially useful, though\n",
    "# not all have been used in this practical --\n",
    "# I'd suggest exploring the use of different \n",
    "# Scalers/Transformers as well as clustering \n",
    "# algorithms...\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import random\n",
    "random.seed(42)    # For reproducibility\n",
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "# Make numeric display a bit neater\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the Scaler(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms  = MinMaxScaler(feature_range=(-1,1))\n",
    "stds = StandardScaler()\n",
    "rbs  = RobustScaler()\n",
    "pts  = PowerTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_ldn(w, b):\n",
    "    \"\"\"\n",
    "    Creates a new figure of a standard size with the \n",
    "    water (w) and boundary (b) layers set up for easy\n",
    "    plotting. Right now this function assumes that you're\n",
    "    looking at London, but you could parameterise it in\n",
    "    other ways ot allow it to work for other areas.\n",
    "    \n",
    "    w: a water layer for London\n",
    "    b: a borough (or other) boundary layer for London\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, figsize=(14, 12))\n",
    "    w.plot(ax=ax, color='#79aef5', zorder=2)\n",
    "    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n",
    "    ax.set_xlim([502000,563000])\n",
    "    ax.set_ylim([155000,201500])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    return fig, ax\n",
    "\n",
    "########################\n",
    "# These may no longer be relevant because of changes to geopandas API\n",
    "\n",
    "def default_cmap(n, outliers=False):\n",
    "    cmap = mpl.cm.get_cmap('viridis_r', n)\n",
    "    colors = cmap(np.linspace(0,1,n))\n",
    "    if outliers:\n",
    "        gray = np.array([225/256, 225/256, 225/256, 1])\n",
    "        colors = np.insert(colors, 0, gray, axis=0)\n",
    "    return ListedColormap(colors)\n",
    "\n",
    "# mappable = ax.collections[-1] if you add the geopandas\n",
    "# plot last.\n",
    "def add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n",
    "    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n",
    "                    boundaries=breaks,\n",
    "                    extend=('min' if outliers else 'neither'), \n",
    "                    spacing='uniform',\n",
    "                    orientation='horizontal',\n",
    "                    fraction=0.05, shrink=0.5, pad=0.05)\n",
    "    cb.set_label(\"Cluster Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Caching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from requests import get\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def cache_data(src:str, dest:str) -> str:\n",
    "    \"\"\"Downloads and caches a remote file locally.\n",
    "    \n",
    "    The function sits between the 'read' step of a pandas or geopandas\n",
    "    data frame and downloading the file from a remote location. The idea\n",
    "    is that it will save it locally so that you don't need to remember to\n",
    "    do so yourself. Subsequent re-reads of the file will return instantly\n",
    "    rather than downloading the entire file for a second or n-th itme.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    src : str\n",
    "        The remote *source* for the file, any valid URL should work.\n",
    "    dest : str\n",
    "        The *destination* location to save the downloaded file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the local location of the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = urlparse(src) # We assume that this is some kind of valid URL \n",
    "    fn  = os.path.split(url.path)[-1] # Extract the filename\n",
    "    dfn = os.path.join(dest,fn) # Destination filename\n",
    "    \n",
    "    # Check if dest+filename does *not* exist -- \n",
    "    # that would mean we have to download it!\n",
    "    if not os.path.isfile(dfn) or os.path.getsize(dfn) < 1:\n",
    "        \n",
    "        print(f\"{dfn} not found, downloading!\")\n",
    "\n",
    "        # Convert the path back into a list (without)\n",
    "        # the filename -- we need to check that directories\n",
    "        # exist first.\n",
    "        path = os.path.split(dest)\n",
    "        \n",
    "        # Create any missing directories in dest(ination) path\n",
    "        # -- os.path.join is the reverse of split (as you saw above)\n",
    "        # but it doesn't work with lists... so I had to google how\n",
    "        # to use the 'splat' operator! os.makedirs creates missing\n",
    "        # directories in a path automatically.\n",
    "        if len(path) >= 1 and path[0] != '':\n",
    "            os.makedirs(os.path.join(*path), exist_ok=True)\n",
    "            \n",
    "        # Download and write the file\n",
    "        with open(dfn, \"wb\") as file:\n",
    "            response = get(src)\n",
    "            file.write(response.content)\n",
    "            \n",
    "        print('Done downloading...')\n",
    "\n",
    "    else:\n",
    "        print(f\"Found {dfn} locally!\")\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: London Data Layers\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\n",
    "ddir  = os.path.join('data','geo') # destination directory\n",
    "water = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\n",
    "boros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\n",
    "green = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n",
    "\n",
    "msoas = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', ddir) )\n",
    "msoas = msoas.to_crs(epsg=27700)\n",
    "\n",
    "# I don't use this in this practical, but it's a\n",
    "# really useful data set that gives you 'names'\n",
    "# for MSOAs that broadly correspond to what most\n",
    "# Londoners would think of as a 'neighbourhood'.\n",
    "msoa_nms = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011-Names.gpkg', ddir) )\n",
    "msoa_nms = msoa_nms.to_crs(epsg=27700)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Reduced Dimensionality MSOA Data\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>\n",
    "\n",
    "You should have this locally from last week, but just in case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'http://orca.casa.ucl.ac.uk'\n",
    "path = '~jreades/data'\n",
    "rddf = gpd.read_feather( cache_data(f'{host}/{path}/Reduced_Dimension_Data.geofeather', ddir) )\n",
    "print(f\"Data frame is {rddf.shape[0]:,} x {rddf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have: `Data frame is 983 x 93`.\n",
    "\n",
    "And below you should see both the components and the dimensions from last week's processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddf.iloc[0:3, -7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get:\n",
    "\n",
    "|        | Component 5 | Component 6 | Component 7 | Borough | Dimension 1 | Dimension 2 | Subregion |\n",
    "| :------| ----------: | -----------:| ----------: | ------: | ----------: | ----------: | --------: |\n",
    "| **E02000001** | 1.44 | 3.95 | -1.52 | City of London | 7.63 | 2.20 | Inner West |\n",
    "| **E02000002** | -0.28 | 0.89 | 0.26 | Barking and Dagenham | 2.59 | 7.01 | Outer East and North East |\n",
    "| **E02000003** | -0.11 | 1.12 | 0.83 | Barking and Dagenham | 3.39 | 6.12 | Outer East and North East |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Listings Data\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>\n",
    "\n",
    "Let's also get the listings data from a few weeks back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = gpd.read_feather( cache_data(f'{host}/{path}/2022-09-10-listings.geofeather', ddir) )\n",
    "listings = listings.to_crs(epsg=27700)\n",
    "listings = listings.drop(columns=['index']).set_index('id')\n",
    "print(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have: `Data frame is 68,741 x 31`.\n",
    "\n",
    "And a quick plot of the price to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.plot(???, cmap='plasma', scheme='quantiles', k=10, \n",
    "              markersize=.5, alpha=0.15, figsize=(10,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Aggregate Listings by MSOA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Join Listings to MSOA\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Medium-to-hard.\n",
    "</div>\n",
    "\n",
    "First, let's link all this using the MSOA Geography that we created last week and a mix or merge and sjoin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoa_listings = gpd.sjoin(???, ???.drop(\n",
    "                        columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM',\n",
    "                                 'USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', \n",
    "                                 'AVHHOLDSZ']), predicate='???').drop(\n",
    "                        columns=[???]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we've added is the MSOA11CD\n",
    "msoa_listings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All being well you should now have:\n",
    "\n",
    "```\n",
    "Index(['listing_url', 'last_scraped', 'name', 'description', 'host_id',\n",
    "       'host_name', 'host_since', 'host_location', 'host_is_superhost',\n",
    "       'host_listings_count', 'host_total_listings_count',\n",
    "       'host_verifications', 'property_type', 'room_type', 'accommodates',\n",
    "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "       'minimum_nights', 'maximum_nights', 'availability_365',\n",
    "       'number_of_reviews', 'first_review', 'last_review',\n",
    "       'review_scores_rating', 'reviews_per_month', 'geometry', 'MSOA11CD'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Price by MSOA \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Medium.\n",
    "</div>\n",
    "\n",
    "Let's calculate the median price by MSOA... Notice that we have to specify the column we want after the `groupby` so the we don't get the median of *every* column returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *m*soa *l*istings *g*rouped *p*rice\n",
    "mlgp = msoa_listings.groupby(???)[???].agg(???) \n",
    "mlgp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like:\n",
    "\n",
    "```\n",
    "MSOA11CD\n",
    "E02000001   196.00\n",
    "E02000002    50.00\n",
    "E02000003    72.00\n",
    "E02000004    52.00\n",
    "E02000005    82.00\n",
    "Name: price, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Room Type by MSOA\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Medium.\n",
    "</div>\n",
    "\n",
    "Now let's calculate the count of room types by MSOA and compare the effects of `reset_index` on the outputs below. And notice too that we can assign the aggregated value to a column name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *m*soa *l*istings *g*rouped *c*ount\n",
    "mlgc = msoa_listings.groupby([???, ???]).listing_url.agg(Count=???)\n",
    "mlgc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something resembling this:\n",
    "\n",
    "| MSOA11CD | room_type | Count<br/>&nbsp;<br />&nbsp; |\n",
    "| -------: | :-------- | --: |\n",
    "| **E02000001** | **Entire home/apt** | 365 |\n",
    "|  | **Hotel room** | 0 |\n",
    "|  | **Private room** | 50 |\n",
    "|  | **Shared room** | 1 |\n",
    "| **E02000002** | **Entire home/apt** | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *m*soa *l*istings *g*rouped *c*ount *r*eset index\n",
    "mlgcr = msoa_listings.groupby([???, ???]).listing_url.agg(Count=???).reset_index()\n",
    "mlgcr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like: \n",
    "\n",
    "|    | MSOA11CD | room_type | Count |\n",
    "| -: | :------- | :-------- | ----: |\n",
    "| 0 | E02000001 | Entire home/apt | 365 |\n",
    "| 1 | E02000001 | Hotel room | 0 |\n",
    "| 2 | E02000001 | Private room | 50 |\n",
    "| 3 | E02000001 | Shared room | 1 |\n",
    "| 4 | E02000002 | Entire home/apt | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Price by Room Type\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Difficulty level</b>: Hard.\n",
    "</div>\n",
    "\n",
    "But perhaps median price/room type would make more sense? And do we want to retain values where there are no listings? For example, there are no hotel rooms listed for E02000001, how do we ensure that these *NAs are dropped*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *m*soa *l*istings *g*rouped *r*oom *p*rice\n",
    "mlgrp = msoa_listings.???(???)[???].agg(???).???.???\n",
    "mlgrp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like:\n",
    "\n",
    "|    | MSOA11CD | room type | price |\n",
    "| -: | :------- | :-------  | ----: |\n",
    "| 0 | E02000001 | Entire home/apt | 200.00 |\n",
    "| 2 | E02000001 | Private room | 67.00 |\n",
    "| 3 | E02000001 | Shared room | 120.00 |\n",
    "| 4 | E02000002 | Entire home/apt | 154.00 |\n",
    "| 6 | E02000002 | Private room | 50.00 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Explore Outlier Per-MSOA Prices\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate, if you try to make sense of the code instead of just running it.\n",
    "</div>\n",
    "\n",
    "Are there MSOAs what *look* like they might contain erroneous data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Plot MSOA Median Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlgp.hist(bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Examine Listings from High-Priced MSOAs\n",
    "\n",
    "Careful, this is showing the *listings* from MSOAs whose median price is above $300/night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoa_listings[\n",
    "    msoa_listings.MSOA11CD.isin(mlgp[mlgp > 300].index)\n",
    "].sort_values(by='price', ascending=False).head(7)[\n",
    "    ['price','room_type','property_type','name','description']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those look fairly 'legit', but what about these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoa_listings[\n",
    "    (msoa_listings.MSOA11CD.isin(mlgp[mlgp > 300].index)) & (msoa_listings.room_type!='Entire home/apt')\n",
    "].sort_values(by='price', ascending=False).head(7)[\n",
    "    ['price','room_type','property_type','name','description']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I don't know that I'd pay $1,675 for a room *anywhere*, no matter how fancy, but if we wanted to be rigorous then we'd have to investigate further: perhaps that's *really* a price/month because the minimum stay is 30 days? And properties in Mayfair and Westminster *are* going to be expensive.\n",
    "\n",
    "On the whole, let's take a *guess* that there are a small number of implausibly high prices for individual units that aren't in very expensive neighbourhoods (the multiple Surbiton listings, for example) and that these are either erroneous/deliberately incorrect, or represent a price that is not per-night. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3 Filter Unlikely Listings\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Difficulty level</b>: Hard.\n",
    "</div>\n",
    "\n",
    "See if you can filter out these less likely listings on the following criteria:\n",
    "\n",
    "1. Listings are priced above $300/night AND\n",
    "2. Room type is not `'Entire home/apt'` AND\n",
    "3. Listings do *not* contain the words: suite, luxury, loft, stunning, prime, historic, or deluxe.\n",
    "\n",
    "I found 901 rows to drop this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_regex = r'(?:suite|luxury|loft|stunning|prime|historic|deluxe)'\n",
    "to_drop = msoa_listings[\n",
    "            ??? & \n",
    "            ??? &\n",
    "            ??? ]\n",
    "print(f\"Have found {to_drop.shape[0]:,} rows to drop on the basis of unlikely per night prices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop.sort_values(by='price', ascending=False)[['price','room_type','name','description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.4 Plot Unlikely Listings\n",
    "\n",
    "Here we use the `plt_ldn` function -- notice how it's designed to return `f,ax` in the same way that `plt.subplots` (which we're already familiar with) does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt_ldn(???, ???)\n",
    "to_drop.plot(column='price', markersize=10, alpha=0.7, cmap='plasma', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.5 ... And Drop\n",
    "\n",
    "The ones listed for the Carlton might be legitimate, but I'm feeling broadly ok with the remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = msoa_listings.drop(index=???)\n",
    "print(f\"Cleaned data has {cleaned.shape[0]:,} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this I had 67,087 rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would normally, at this point, spend quite a bit of time validating this cleaning approach, but right now we're going to take a rough-and-ready approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.5.6 Questions\n",
    "\n",
    "- What data type did [Task 2.2](#Task-2.2:-Price-by-MSOA) return?\n",
    "- What is the function of `reset_index()` in [Task 2.3](#Task-2.:3-Room-Type-by-MSOA) and when might you choose to reset (or not)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Pivot Tables & 'Wide Data'\n",
    "\n",
    "The `group_by` operation is *one* way to organise and aggregate our data, but pivot tables are a *second* common way to achieve this. We typically use a pivot table to go from long to wide data frames -- it's often seen as one of Excel's main benefits, but Pandas can do that too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Create Pivot Table\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Difficulty level</b>: Hard.\n",
    "</div>\n",
    "\n",
    "We can make use of the pivot table function to generate counts by MSOA in a 'wide' format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = cleaned.groupby(\n",
    "                ['MSOA11CD','room_type']\n",
    "        ).listing_url.agg(Count='count').reset_index().pivot(???)\n",
    "pivot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formatting will look a tiny bit different, but you should get something like this: \n",
    "\n",
    "|    |          |       |       |       | Count |\n",
    "| -: | -------: | ----: | ----: | ----: | ----: |\n",
    "| **room_type** | **Entire home/apt** | **Hotel room** | **Private room** | **Shared room** |\n",
    "| **MSOA11CD**. |  |  |  |  |  | \n",
    "| **E02000001** | 365 | 0 | 48 | 1 | \n",
    "| **E02000002** | 2 | 0 | 6 | 0 |\n",
    "| **E02000003** | 8 | 0 | 10 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Check Counts\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned[cleaned.room_type=='Entire home/apt'].listing_url.count())\n",
    "print(cleaned[cleaned.room_type=='Private room'].listing_url.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Tidy & Normalise\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>\n",
    "\n",
    "My instinct at this point is that, looking at the pivot table, we see quite different levels of Airbnb penetration and it is hard to know how handle this difference: share would be unstable because of the low counts in some places and high counts in others; a derived variable that tells us something about density or mix could be interesting (e.g. HHI or LQ) but wouldn't quite capture the pattern of mixing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Tidy\n",
    "\n",
    "Personally, based on the room type counts above I think we can drop Hotel Rooms and Shared Rooms from this since the other two categories are *so* dominant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the column index\n",
    "pivot.columns = ['Entire home/apt','Hotel room','Private room','Shared room']\n",
    "# Drop the columns\n",
    "pivot.drop(???)\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have only the **Entire home/apt** and **Private room** columns now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_norm = pd.DataFrame(index=pivot.index)\n",
    "for c in pivot.columns.to_list():\n",
    "    # Power Transform\n",
    "    pivot_norm[c] = pts.???(pivot[c].to_numpy().reshape(???,???))\n",
    "\n",
    "pivot_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like:\n",
    "\n",
    "|          | Entire home/apt | Private room |\n",
    "| :------- | --------------: | -----------: |\n",
    "| **MSOA11CD** |  |  |\n",
    "| **E02000001** | 2.19 | 1.04 |\n",
    "| **E02000002** | -1.41 | -0.96 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnm = pd.merge(msoas.set_index(???), pivot_norm, left_index=???, right_index=???)\n",
    "pnm.plot(column='Entire home/apt', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: PCA\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate, though you might find the questions hard.\n",
    "</div>\n",
    "\n",
    "You can merge the output of this next step back on to the `rddf` data frame as part of a clustering process, though we'd really want to do some more thinking about what this data *means* and what transformations we'd need to do in order to make them *meaningful*. \n",
    "\n",
    "For instance, if we went back to last week's code, we could have appended this InsideAirbnb data *before* doing the dimensionality reduction, or we could apply it now to create a new measure that could be used as a separate part of the clustering process together with the reduced dimensionality of the demographic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Perform Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcomp = PCA(n_components=???, random_state=42)\n",
    "rd    = pcomp.???(pivot_norm)\n",
    "print(f\"The explained variance of each component is: {', '.join([f'{x*100:.2f}%' for x in pcomp.explained_variance_ratio_])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a data frame to enable the merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_pca = pd.DataFrame(\n",
    "                {'Airbnb Component 1': mms.fit_transform(rd[:,1].reshape(-1,1)).reshape(1,-1)[0]}, \n",
    "                index=???)\n",
    "\n",
    "airbnb_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like:\n",
    "|       | Airbnb Component 1 |\n",
    "| :---- | ----: |\n",
    "| **MSOA11CD** |     |\n",
    "| **E02000001** | 0.40 |\n",
    "| **E02000002** | -0.39 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcanm = pd.merge(msoas.set_index(???), ???, left_index=True, right_index=???)\n",
    "pcanm.plot(column='Airbnb Component 1', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Write to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Set from merge\n",
    "rs = pd.merge(rddf, airbnb_pca, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the PCA, UMAP, and Airbnb outputs for clustering and append rescaled price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the reducded dimensionality data frame with the PCA-reduced Airbnb data\n",
    "# to create the *cl*uster *d*ata *f*rame\n",
    "cldf = pd.merge(rddf.loc[:,'Component 1':], airbnb_pca, \n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "# Append median price from cleaned listings grouped by MSOA too!\n",
    "s1 = cleaned.groupby(by='MSOA11CD').price.agg('median')\n",
    "cldf['median_price'] = pd.Series(np.squeeze(mms.fit_transform(s1.values.reshape(-1,1))), index=s1.index)\n",
    "\n",
    "# Append mean price from cleaned listings grouped by MSOA too!\n",
    "s2 = cleaned.groupby(by='MSOA11CD').price.agg('mean')\n",
    "cldf['mean_price'] = pd.Series(np.squeeze(mms.fit_transform(s2.values.reshape(-1,1))), index=s2.index)\n",
    "\n",
    "cldf.drop(columns=['Subregion','Borough'], inplace=True)\n",
    "\n",
    "cldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Questions\n",
    "\n",
    "- Have a think about why you might want to keep the Airbnb data separate from the MSOA data when doing PCA (or any other kind of dimensionality reduction)!\n",
    "- Why *might* it be interesting to add *both* mean and median MSOA prices to the clustering process? Here's a hint (but it's *very* subtle): `sns.jointplot(x=s1, y=s2, s=15, alpha=0.6)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. First K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Perform Clustering\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nm   = 'KMeans' # Clustering name\n",
    "k_pref = 3 # Number of clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=???, n_init=25, random_state=42).fit(cldf.drop(columns=['Dimension 1','Dimension 2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_) # The results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Save Clusters to Data Frame\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Write Series and Assign\n",
    "\n",
    "Now capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (`rs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs[c_nm] = pd.Series(???, index=cldf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Histogram of Cluster Members\n",
    "\n",
    "How are the clusters distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=rs, x=???, bins=???);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Map Clusters\n",
    "\n",
    "And here's a map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boros)\n",
    "fig.suptitle(f\"{c_nm} Results (k={???})\", fontsize=20, y=0.92)\n",
    "rs.plot(column=???, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Questions\n",
    "\n",
    "- What critical assumption did we make when running this analysis?\n",
    "- Why did I *not* use the UMAP dimensions here?\n",
    "- Why do we have the `c_nm='kMeans'` when we *know* what kind of clustering we're doing?\n",
    "- Does this look like a *good* clustering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Second K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: What's the 'Right' Number of Clusters?\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate.\n",
    "</div>\n",
    "\n",
    "There's more than one way to find the 'right' number of clusters. In Singleton's _Geocomputation_ chapter they use WCSS to pick the 'optimal' number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2...n) and then look for a 'knee' (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don't always see a knee.\n",
    "\n",
    "Another way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the 'quality' of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it's based on Partitioning Around the Medoid (PAM).\n",
    "\n",
    "Either way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means cluster data frame\n",
    "kcldf = cldf.drop(columns=['Dimension 1','Dimension 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Repeated Clustering\n",
    "\n",
    "Let's try clustering across a wider range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# For resolutions of 'k' in the range 2..40\n",
    "for k in range(2,41):\n",
    "    \n",
    "    #############\n",
    "    # Do the clustering using the main columns\n",
    "    kmeans = KMeans(n_clusters=????, n_init=25, random_state=42).fit(???)\n",
    "    \n",
    "    # Calculate the overall silhouette score\n",
    "    silhouette_avg = silhouette_score(???, kmeans.???)\n",
    "    \n",
    "    y.append(k)\n",
    "    x.append(silhouette_avg)\n",
    "    \n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Plot Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n",
    "\n",
    "plt.plot(y, x)\n",
    "plt.gca().xaxis.grid(True);\n",
    "plt.gcf().suptitle(\"Average Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>&#9888; Note</b>: Had we used the UMAP dimensions here you'd likely see more instability in the silhouette plot because the distribution is not remotely Gaussian, though a lot depends on the magnitude of the columns and the number of UMAP vs. PCA components.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the largest average silhouette score to determine the 'natural' number of clusters in the data, but that that's only if we don't have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value... Again, we're now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Final Clustering \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>\n",
    "\n",
    "So although we should probably pick the largest silhouette scores, that's `k=3` which kind of defeats the purpose of clustering in the first place. In the absence of a _compelling_ reason to pick 2 or 3 clusters, let's have a closer look at the _next_ maximum silhouetted score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Perform Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_pref=???\n",
    "\n",
    "#############\n",
    "# Do the clustering using the main columns\n",
    "kmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(kcldf)\n",
    "\n",
    "# Convert to a series\n",
    "s = pd.Series(kmeans.???, index=kcldf.???, name=c_nm)\n",
    "\n",
    "# We do this for plotting\n",
    "rs[c_nm] = s\n",
    "    \n",
    "# Calculate the overall silhouette score\n",
    "silhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n",
    "\n",
    "# Calculate the silhouette values\n",
    "sample_silhouette_values = silhouette_samples(kcldf, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Plot Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(9, 5)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1\n",
    "ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
    "    \n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, kcldf.shape[0] + (k_pref + 1) * 10])\n",
    "    \n",
    "y_lower = 10\n",
    "    \n",
    "# For each of the clusters...\n",
    "for i in range(k_pref):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[kmeans.labels_ == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "    # Set the color ramp\n",
    "    color = plt.cm.Spectral(i/k_pref)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, ith_cluster_silhouette_values,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed --\n",
    "    # we can only do this for the first two dimensions\n",
    "    # so we may not see fully what is causing the \n",
    "    # resulting assignment\n",
    "    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k_pref)\n",
    "    ax2.scatter(kcldf[kcldf.columns[0]], kcldf[kcldf.columns[1]], \n",
    "                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"Visualization of the clustered data\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette results for KMeans clustering \"\n",
    "                \"with %d clusters\" % k_pref),\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>&#9888; Stop</b>: Make sure that you understand how the silhouette plot and value work, and why your results <em>may</em> diverge from mine.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Map Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boros)\n",
    "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
    "???.plot(column=???, ax=ax, linewidth=0, zorder=0, categorical=???, legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 5.3: 'Representative' Centroids\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate since, conceptually, there's a lot going on.\n",
    "</div>\n",
    "\n",
    "To get a sense of how these clusters differ we can try to extract 'representative' centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There's also a k-medoids clustering approach built around the median centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are columns that we want to suppress from our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n",
    "             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n",
    "             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n",
    "             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n",
    "             'Airbnb Component 1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample of the full range of numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of these columns for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty data frame with the columns we'll need\n",
    "centroids = pd.DataFrame(columns=???)\n",
    "\n",
    "# For each cluster...\n",
    "for k in sorted(rs[c_nm].unique()):\n",
    "    print(f\"Processing cluster {k}\")\n",
    "    \n",
    "    # Select rows where the cluster name matches the cluster number\n",
    "    clust = rs[rs[c_nm]==k]\n",
    "    \n",
    "    # Append the means to the centroids data frame\n",
    "    centroids.loc[k] = clust[cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
    "for i in range(0,len(centroids.index)):\n",
    "    row = centroids.iloc[i,:]\n",
    "    for r in row.index:\n",
    "        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n",
    "        centroids_long = pd.concat([centroids_long, d], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
    "g = g.map(plt.bar, \"Cluster\", \"Std. Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. DBSCAN\n",
    "\n",
    "For what it's worth, I've had _enormous_ trouble with DBSCAN and this kind of data. I don't think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the PCA dimensions\n",
    "cldf2 = cldf.loc[:,'Dimension 1':].copy()\n",
    "for c in [x for x in cldf.columns.to_list() if x.startswith('Dimension ')]:\n",
    "    cldf2[c] = pd.Series(np.squeeze(mms.fit_transform(cldf2[c].to_numpy().reshape(-1,1))), index=cldf2.index)\n",
    "cldf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.1: Work out the Neighbour Distance\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate.\n",
    "</div>\n",
    "\n",
    "We normally look for some kind of 'knee' to set the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=6).fit(???)\n",
    "distances, indices = nbrs.kneighbors(???)\n",
    "\n",
    "distances = np.sort(???, axis=0)\n",
    "distances = distances[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2: Derive Approximate Knee\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Difficulty level</b>: Low.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import knee_locator\n",
    "\n",
    "kn = knee_locator.KneeLocator(np.arange(distances.shape[0]), distances, S=12,\n",
    "                              curve='???', direction='???')\n",
    "print(f\"Knee detected at: {kn.???}\")\n",
    "kn.plot_knee()\n",
    "kn.plot_knee_normalized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best guess at epsilon for DBSCAN is {distances[kn.knee]:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.3: Explore Epsilons\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate.\n",
    "</div>\n",
    "\n",
    "There are two values that need to be specified: `eps` and `min_samples`. Both seem to be set largely by trial and error, though we can use the above result as a target. It's easiest to set `min_samples` first since that sets a floor for your cluster size and then `eps` is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Iterate Over Range\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>&#9888; Warning</b>: Depending on the data volume, this next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn't) the graph above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nm = 'DBSCAN'\n",
    "\n",
    "# Make numeric display a bit neater\n",
    "pd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n",
    "\n",
    "el  = []\n",
    "\n",
    "max_clusters  = 10\n",
    "cluster_count = 1\n",
    "\n",
    "iters = 0\n",
    "\n",
    "for e in np.arange(0.025, 0.76, 0.01): # <- You might want to adjust these!\n",
    "    \n",
    "    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n",
    "    \n",
    "    # Run the clustering\n",
    "    dbs = DBSCAN(eps=???, min_samples=cldf2.shape[1]+1).fit(cldf2)\n",
    "    \n",
    "    # See how we did\n",
    "    s = pd.Series(dbs.???, index=cldf2.???, name=c_nm)\n",
    "    \n",
    "    row = [e]\n",
    "    data = s.value_counts()\n",
    "    \n",
    "    for c in range(-1, max_clusters+1):\n",
    "        try:\n",
    "            if np.isnan(data[c]):\n",
    "                row.append(None)\n",
    "            else: \n",
    "                row.append(data[c])\n",
    "        except KeyError:\n",
    "            row.append(None)\n",
    "    \n",
    "    el.append(row)\n",
    "    iters+=1\n",
    "\n",
    "edf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n",
    "\n",
    "# Make numeric display a bit neater\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Examine Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf.head() # Notice the -1 cluster for small epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_long = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n",
    "\n",
    "for i in range(0,len(edf.index)):\n",
    "    row = edf.iloc[i,:]\n",
    "    for c in range(1,len(edf.columns.values)):\n",
    "        if row[c] != None and not np.isnan(row[c]):\n",
    "            d = pd.DataFrame({'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}, index=[1])\n",
    "            epsilon_long = pd.concat([epsilon_long, d], ignore_index=True)\n",
    "\n",
    "epsilon_long['Count'] = epsilon_long.Count.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3 Plot Cluster Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.lineplot(data=epsilon_long, x='Epsilon', y='Count', hue='Cluster');\n",
    "plt.vlines(x=distances[kn.knee], ymin=0, ymax=epsilon_long.Count.max(), color=(1, .7, .7, .8), linestyles='dashed')\n",
    "plt.gcf().suptitle(f\"Cluster sizes for various realisations of Epsilon\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.4: Final Clustering\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Difficulty level</b>: Moderate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1: Perform Clustering\n",
    "\n",
    "Use the value from kneed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = DBSCAN(eps=???, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\n",
    "s = pd.Series(dbs.???, index=cldf2.???, name=c_nm)\n",
    "rs[c_nm] = s\n",
    "print(s.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2: Map Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boros)\n",
    "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
    "rs.plot(column=???, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.3 'Representative' Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n",
    "             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n",
    "             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n",
    "             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n",
    "             'Airbnb Component 1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample of the full range of numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of these columns for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty data frame with the columns we'll need\n",
    "centroids = pd.DataFrame(columns=cols)\n",
    "\n",
    "# For each cluster...\n",
    "for k in sorted(rs[c_nm].unique()):\n",
    "    print(f\"Processing cluster {k}\")\n",
    "    \n",
    "    # Select rows where the cluster name matches the cluster number\n",
    "    clust = rs[rs[c_nm]==k]\n",
    "    \n",
    "    # Append the means to the centroids data frame\n",
    "    centroids.loc[k] = clust[cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unclustered records (-1)\n",
    "centroids.drop(labels=[-1], axis=0, inplace=True)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
    "for i in range(0,len(centroids.index)):\n",
    "    row = centroids.iloc[i,:]\n",
    "    for r in row.index:\n",
    "        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n",
    "        centroids_long = pd.concat([centroids_long, d], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
    "g = g.map(plt.bar, \"Cluster\", \"Std. Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Self-Organising Maps\n",
    "\n",
    "SOMs offer a third type of clustering algorithm. They are a relatively 'simple' type of neural network in which the 'map' (of the SOM) adjusts to the data. We're not going to do this in *this* practical, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or t-SNE or UMAP. Tehy are used quite a lot for text-clustering using keywords (where you have high-dimensionality).\n",
    "\n",
    "There are [a lot of SOM implementations in Python](https://www.google.com/search?q=python+self-organizing+map) but the one I used to use, called SOMPY, appears to have [been abandonned](https://github.com/sevamoo/SOMPY/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Classification\n",
    "\n",
    "And now for something completely different! This is section is **completely optional**, but I thought that you might find it helpful to have a look at how *supervised* learning (classification) differs from *unsupervised* learning (clustering). Here we're going to perform a fairly straightforward classification: predicting the `room_type` for randomly-selected listings. Of course we know the true answer, but this is for demonstration purposes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.1: Additional Setup\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Difficulty level</b>: Hard, as I've left out quite a bit of code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Set Up Data\n",
    "\n",
    "I'm taking a fairly brutal approach here: anything that is not inherently numeric is gone (bye, bye, text), and I'm not bothering to convert implicitly numeric values either: dates could be converted to 'months since last review', for instance, while amenities could be One-Hot Encoded after some pruning of rare amenities. This leaves us with a much smaller number of columns to feed *in* to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cleaned columns: {', '.join(cleaned.columns.to_list())}.\")\n",
    "classifier_in = cleaned.drop(columns=['listing_url','last_scraped','name','description',\n",
    "                                      'host_name', 'host_location', 'property_type', \n",
    "                                      'bathrooms_text', 'amenities', 'geometry', 'MSOA11CD',\n",
    "                                      'host_since', 'first_review', 'last_review',\n",
    "                                      'host_verifications', 'review_scores_rating',\n",
    "                                      'reviews_per_month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 Remove NAs\n",
    "\n",
    "Not all classifiers have this issue, but some will struggle to make predictions (or not be able to do so at all) if there are NAs in the data set. The classifier we're using can't deal with NAs, so we have to strip these out, but before we do let's check the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_in.???.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely drop these now, and you should end up with about 63,162 rows to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_in = classifier_in.dropna(???)\n",
    "print(f\"Now have {classifier_in.shape[0]:,} rows of data to work with (down from {cleaned.shape[0]:,}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(f\"Classifier training columns: {', '.join(classifier_in.columns.to_list())}.\")\n",
    "classifier_in.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.4 Remap Non-Numeric Columns\n",
    "\n",
    "We do still have a couple of non-numeric columns to deal with: booleans and the thing we're actually trying to predict (the room type)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_in[???] = classifier_in.???.replace({True:???, False:???}).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "classifier_in['room_class'] = le.???(classifier_in.???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check: we should only have one type per class and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_in.groupby(by=['room_type','room_class']).host_id.agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.2: Random Forest Classification\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Difficulty level</b>: Hard.\n",
    "</div>\n",
    "\n",
    "We're going to use a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) but the nice thing about `sklearn` is that you can quite easily swap in other classifiers if you'd like to explore further. This is one *big* advantage of Python over R in my book: whereas R tends to get new algorithms *first*, they are often implemented independently by many people and you can end up with incompatible data structures that require a lot of faff to reorganise for a different algorithm. Python is a bit more 'managed' and the dominance of `numpy` and `sklearn` and `pandas` means that people have an incentive to contribute to this library or, if it's genuinely novel, to create an implementation that works *like* it would if it were part of `sklearn`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(???, test_size=0.2, random_state=42)\n",
    "print(f\"Train contains {train.shape[0]:,} records.\")\n",
    "print(f\"Test contains {test.shape[0]:,} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.???\n",
    "X_train = train.drop(columns=['room_class','room_type'])\n",
    "\n",
    "y_test  = test.???\n",
    "X_test  = test.drop(columns=['room_class','room_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 Classifier Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    max_depth=8,\n",
    "    min_samples_split=7,\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3 Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(???, ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rfc.predict(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.3 Validate\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Difficulty level</b>: Hard.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = pd.DataFrame(confusion_matrix(???, ???))\n",
    "c_matrix.index = le.???(c_matrix.index)\n",
    "c_matrix.columns = le.???(c_matrix.columns)\n",
    "c_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2 Feature Importance \n",
    "\n",
    "Compare the Random Forest's built-in 'feature importance' with Permutation Feature Importance as [documented here](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi_importances = pd.Series(\n",
    "    rfc.feature_importances_, index=rfc.feature_names_in_\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "ax = mdi_importances.plot.barh()\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.3 Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    rfc, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_test.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (Test Set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.4 Shapely Values\n",
    "\n",
    "Shapely values are a big part of [explainable AI](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html) and they work (very broadly) by permuting the data to explore how sensitive the predictions made by the model are to the results that you see. For these we need to install two libraries: `shap` (to do the heavy lifting) and `slicer` to deal with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1 Install Libraries\n",
    "\n",
    "You could also wrap this in a `try... except` if you want to write reproducible code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install slicer shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2 Check for Data Types\n",
    "\n",
    "You are looking for anything *other* than `int64` or `float64` for the most part. Boolean should be fine, but pandas' internal, nullable integer type will give you a `ufunc` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[???] = X_test.???.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3 Plot Partial Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.partial_dependence_plot(\n",
    "    \"price\", rfc.predict, X_test, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.4 Calculate Shapely Values\n",
    "\n",
    "This can take a *long* time: 4-5 hours (!!!) without developing a *strategy* for tackling it. See the [long discussion here](https://github.com/slundberg/shap/issues/77). I've taken the approach of subsetting the data substantially (the model is already trained so it won't impact the model's predictions) with a 20% fraction of the test data and an explainer sample of 5%. On my laptop the 'Permutation explainer' stage took about 14 minutes, but your results may obviously be rather different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsample = shap.utils.sample(X_test.sample(frac=0.2, random_state=41), 10)\n",
    "explainer = shap.Explainer(rfc.???, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the shap values for the 5% sample from `X_test`.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>&#9888; Warning</b>: This next block is the one that takes a long time to run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_test.sample(frac=.05, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.5 Single Observation\n",
    "\n",
    "Now you can take any random record (`sample_ind`) and produce a shap plot to show the role that each attribute played in its classification. Note that getting these plots to save required [some searching on GitHub](https://github.com/slundberg/shap/issues/153)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ind=250\n",
    "shap.plots.waterfall(shap_values[sample_ind], max_display=14, show=False);\n",
    "plt.title(f\"Shapely values for observation #{sample_ind} ({X_test.sample(frac=.05, random_state=42).iloc[sample_ind].name})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('practical-09-waterfall.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shapely Feature Plot for Feature 250](https://github.com/jreades/fsds/raw/master/practicals/img/practical-09-waterfall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.6 All Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, show=False)\n",
    "plt.title(f\"Shapely Swarm Plot for Sample\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('practical-09-swarm.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shapely Swarm Plot](https://github.com/jreades/fsds/raw/master/practicals/img/practical-09-swarm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the appropriate eps value: [Nearest Neighbour Distance Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Nearest-Neighbor-Distance-Functions) or [Interevent Distance Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Interevent-Distance-Functions)\n",
    "- [Clustering Points](https://darribas.org/gds_course/content/bH/lab_H.html#clusters-of-points)\n",
    "- [Regionalisation algorithms with Aglomerative Clustering](https://darribas.org/gds_course/content/bG/lab_G.html#regionalization-algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've reached the end, you're done... \n",
    "\n",
    "Er, no. This is barely scratching the surface! I'd suggest that you go back through the above code and do three things:\n",
    "1. Add a lot more comments to the code to ensure that really have understood what is going on.\n",
    "2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change.\n",
    "3. Try outputting additional plots that will help you to understand the _quality_ of your clustering results (e.g. what _is_ the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\n",
    "\n",
    "If all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\n",
    "\n",
    "See: [Introduction to Machine Learning with Scikit-Learn](http://www.slideshare.net/BenjaminBengfort/introduction-to-machine-learning-with-scikitlearn)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
